# Copyright (c) OpenMMLab. All rights reserved.
import math
import warnings

import torch
import torch.nn as nn
import torch.utils.checkpoint as cp
from mmcv.cnn import build_norm_layer
from mmcv.cnn.bricks.transformer import build_dropout
from mmengine.model import BaseModule, ModuleList
from mmengine.model.weight_init import (constant_init, normal_init,
                                        trunc_normal_init)

from mmseg.registry import MODELS
from ..utils import PatchEmbed, nlc_to_nchw, nchw_to_nlc

from .mit import MixFFN
from ..utils.ex_attention import EX_Module2, EX_Module_noself, EX_Module_noselect_par, EX_Module_noselect_seq, EX_Module_onlyselect
from ..utils.inverted_residual import InvertedResidual
from ..utils.PSA import PSA_p
from ..utils.se_layer import SELayer

class ExAttention(nn.Module):
    def __init__(self,
                 embed_dims,
                 dropout_layer=None,
                 ex_module=EX_Module2,
                 norm_cfg=dict(type='LN', eps=1e-6)):
        super(ExAttention, self).__init__()
        self.dropout_layer = build_dropout(dropout_layer)
        self.ex_module = ex_module(in_channels=embed_dims,
                                   channels=embed_dims,
                                   norm_cfg=norm_cfg)

    def forward(self, x, hw_shape):
        x = nlc_to_nchw(x, hw_shape)
        ex_out = nchw_to_nlc(self.ex_module(x))

        return self.dropout_layer(ex_out)

class PSAAttention(nn.Module):
    def __init__(self,
                 embed_dims,
                 dropout_layer=None,
                 ex_module=None,
                 norm_cfg=dict(type='LN')):
        super(PSAAttention, self).__init__()
        self.dropout_layer = build_dropout(dropout_layer)
        self.psa_module = PSA_p(in_channels=embed_dims,
                                planes=embed_dims)

    def forward(self, x, hw_shape):
        x = nlc_to_nchw(x, hw_shape)
        ex_out = nchw_to_nlc(self.psa_module(x))

        return self.dropout_layer(ex_out)

class SEAttention(nn.Module):
    def __init__(self,
                 embed_dims,
                 dropout_layer=None,
                 ex_module=None,
                 norm_cfg=dict(type='LN')):
        super(SEAttention, self).__init__()
        self.dropout_layer = build_dropout(dropout_layer)
        self.se_layer = SELayer(in_channels=embed_dims)

    def forward(self, x, hw_shape):
        x = nlc_to_nchw(x, hw_shape)
        ex_out = nchw_to_nlc(self.se_layer(x))

        return self.dropout_layer(ex_out)

class ExTransformerEncoderLayer(BaseModule):
    """Implements one encoder layer in Segformer.

    Args:
        embed_dims (int): The feature dimension.
        num_heads (int): Parallel attention heads.
        feedforward_channels (int): The hidden dimension for FFNs.
        drop_rate (float): Probability of an element to be zeroed.
            after the feed forward layer. Default 0.0.
        attn_drop_rate (float): The drop out rate for attention layer.
            Default 0.0.
        drop_path_rate (float): stochastic depth rate. Default 0.0.
        qkv_bias (bool): enable bias for qkv if True.
            Default: True.
        act_cfg (dict): The activation config for FFNs.
            Default: dict(type='GELU').
        norm_cfg (dict): Config dict for normalization layer.
            Default: dict(type='LN').
        batch_first (bool): Key, Query and Value are shape of
            (batch, n, embed_dim)
            or (n, batch, embed_dim). Default: False.
        init_cfg (dict, optional): Initialization config dict.
            Default:None.
        sr_ratio (int): The ratio of spatial reduction of Efficient Multi-head
            Attention of Segformer. Default: 1.
        with_cp (bool): Use checkpoint or not. Using checkpoint will save
            some memory while slowing down the training speed. Default: False.
    """

    def __init__(self,
                 index,
                 embed_dims,
                 token_mixer,
                 feedforward_channels,
                 ex_module=EX_Module2,
                 drop_rate=0.,
                 drop_path_rate=0.,
                 act_cfg=dict(type='GELU'),
                 norm_cfg=dict(type='LN', eps=1e-6),
                 with_cp=False):
        super(ExTransformerEncoderLayer, self).__init__()

        # The ret[0] of build_norm_layer is norm name.
        self.norm1 = build_norm_layer(norm_cfg, embed_dims)[1]
        self.index = index
        if index < 2:
            self.tokenmixer = InvertedResidual(
                in_channels=embed_dims,
                out_channels=embed_dims,
                stride=1,
                expand_ratio=2,
                act_cfg=act_cfg)
        else:
            self.tokenmixer = token_mixer(
                embed_dims=embed_dims,
                dropout_layer=dict(type='DropPath', drop_prob=drop_path_rate),
                ex_module=ex_module,
                norm_cfg=norm_cfg)

        # The ret[0] of build_norm_layer is norm name.
        self.norm2 = build_norm_layer(norm_cfg, embed_dims)[1]

        self.ffn = MixFFN(
            embed_dims=embed_dims,
            feedforward_channels=feedforward_channels,
            ffn_drop=drop_rate,
            dropout_layer=dict(type='DropPath', drop_prob=drop_path_rate),
            act_cfg=act_cfg)

        self.with_cp = with_cp

    def forward(self, x, hw_shape):

        def _inner_forward(x):
            if self.index < 2:
                x = nlc_to_nchw(self.norm1(x), hw_shape)
                x = self.tokenmixer(x)
                x = nchw_to_nlc(x)
            else:
                x = self.tokenmixer(self.norm1(x), hw_shape)
            x = self.ffn(self.norm2(x), hw_shape, identity=x)
            return x

        if self.with_cp and x.requires_grad:
            x = cp.checkpoint(_inner_forward, x)
        else:
            x = _inner_forward(x)
        return x


@MODELS.register_module()
class ExMixVisionTransformer(BaseModule):
    def __init__(self,
                 in_channels=3,
                 embed_dims=64,
                 token_mixers=ExAttention,
                 ex_module=EX_Module2,
                 num_stages=4,
                 num_layers=[3, 4, 6, 3],
                 num_heads=[1, 2, 4, 8],
                 patch_sizes=[7, 3, 3, 3],
                 strides=[4, 2, 2, 2],
                 out_indices=(0, 1, 2, 3),
                 mlp_ratio=4,
                 drop_rate=0.,
                 drop_path_rate=0.,
                 act_cfg=dict(type='GELU'),
                 norm_cfg=dict(type='LN', eps=1e-6),
                 pretrained=None,
                 init_cfg=None,
                 with_cp=False):
        super(ExMixVisionTransformer, self).__init__(init_cfg=init_cfg)

        assert not (init_cfg and pretrained), \
            'init_cfg and pretrained cannot be set at the same time'
        if isinstance(pretrained, str):
            warnings.warn('DeprecationWarning: pretrained is deprecated, '
                          'please use "init_cfg" instead')
            self.init_cfg = dict(type='Pretrained', checkpoint=pretrained)
        elif pretrained is not None:
            raise TypeError('pretrained must be a str or None')

        self.embed_dims = embed_dims
        self.num_stages = num_stages
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.patch_sizes = patch_sizes
        self.strides = strides
        self.with_cp = with_cp

        self.out_indices = out_indices
        assert max(out_indices) < self.num_stages

        # transformer encoder
        dpr = [
            x.item()
            for x in torch.linspace(0, drop_path_rate, sum(num_layers))
        ]  # stochastic num_layer decay rule

        cur = 0
        self.layers = ModuleList()
        for i, num_layer in enumerate(num_layers):
            embed_dims_i = embed_dims * num_heads[i]
            patch_embed = PatchEmbed(
                in_channels=in_channels,
                embed_dims=embed_dims_i,
                kernel_size=patch_sizes[i],
                stride=strides[i],
                padding=patch_sizes[i] // 2,
                norm_cfg=norm_cfg)
            layer = ModuleList([
                ExTransformerEncoderLayer(
                    index=i,
                    embed_dims=embed_dims_i,
                    token_mixer=token_mixers,
                    feedforward_channels=mlp_ratio * embed_dims_i,
                    ex_module=ex_module,
                    drop_rate=drop_rate,
                    drop_path_rate=dpr[cur + idx],
                    act_cfg=act_cfg,
                    norm_cfg=norm_cfg,
                    with_cp=with_cp) for idx in range(num_layer)
            ])
            in_channels = embed_dims_i
            # The ret[0] of build_norm_layer is norm name.
            norm = build_norm_layer(norm_cfg, embed_dims_i)[1]
            self.layers.append(ModuleList([patch_embed, layer, norm]))
            cur += num_layer

    def init_weights(self):
        if self.init_cfg is None:
            for m in self.modules():
                if isinstance(m, nn.Linear):
                    trunc_normal_init(m, std=.02, bias=0.)
                elif isinstance(m, nn.LayerNorm):
                    constant_init(m, val=1.0, bias=0.)
                elif isinstance(m, nn.Conv2d):
                    fan_out = m.kernel_size[0] * m.kernel_size[
                        1] * m.out_channels
                    fan_out //= m.groups
                    normal_init(
                        m, mean=0, std=math.sqrt(2.0 / fan_out), bias=0)
        else:
            super(ExMixVisionTransformer, self).init_weights()

    def forward(self, x):
        outs = []

        for i, layer in enumerate(self.layers):
            x, hw_shape = layer[0](x)
            for block in layer[1]:
                x = block(x, hw_shape)
            x = layer[2](x)
            x = nlc_to_nchw(x, hw_shape)
            if i in self.out_indices:
                outs.append(x)

        return outs

@MODELS.register_module()
class PSAFormer(ExMixVisionTransformer):
    def __init__(self, **kwargs):
        super(PSAFormer, self).__init__(
            token_mixers=PSAAttention,
            ex_module=None,
            **kwargs)

@MODELS.register_module()
class SEFormer(ExMixVisionTransformer):
    def __init__(self, **kwargs):
        super(SEFormer, self).__init__(
            token_mixers=SEAttention,
            ex_module=None,
            **kwargs)

@MODELS.register_module()
class ExFormer_NoSelf(ExMixVisionTransformer):
    def __init__(self, **kwargs):
        super(ExFormer_NoSelf, self).__init__(
            ex_module=EX_Module_noself,
            **kwargs)

@MODELS.register_module()
class ExFormer_NoSlct_Seq(ExMixVisionTransformer):
    def __init__(self, **kwargs):
        super(ExFormer_NoSlct_Seq, self).__init__(
            ex_module=EX_Module_noselect_seq,
            **kwargs)

@MODELS.register_module()
class ExFormer_NoSlct_Par(ExMixVisionTransformer):
    def __init__(self, **kwargs):
        super(ExFormer_NoSlct_Par, self).__init__(
            ex_module=EX_Module_noselect_par,
            **kwargs)

@MODELS.register_module()
class ExFormer_Onlyselect(ExMixVisionTransformer):
    def __init__(self, **kwargs):
        super(ExFormer_Onlyselect, self).__init__(
            ex_module=EX_Module_onlyselect,
            **kwargs)

